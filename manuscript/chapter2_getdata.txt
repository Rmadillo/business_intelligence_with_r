# Chapter 2: Getting Data

- Working with Files
- Working with Databases
- Getting Data from the Web
- Creating Fake Data to Test Code
- Writing Files to Disk

The tried-and-true method for bringing data into R is via the humble csv file, and in many cases it will be the default approach. But in most professional environments, you'll encounter a wide variety of file formats and production needs, so having a cheat sheet of data import methods is useful.

There are a wide variety of packages and functions to import data from files, but a few provide nearly all the functionality you'll need:

{width="90%"}
| File Type | Function | Package |
| --------- | -------- | ------- |
| csv file | `read.table("file", sep=",", ...)`  | base |
|  | `read.csv("file", ...)` |  | 
| Flat file | `read.table("file", sep=" ", ...)` | base |
|  | `fread("file", sep=" ", ...)` | data.table |
| Excel | `read_excel("Excelfile", sheet, ...)` | readxl |
|  | `read.xls("Excelfile",sheet, ...)` | gdata |
|  | `readWorksheet("Excelfile", sheet, ...)` | XLConnect |
| XML | `xmlToDataFrame("xmlfile", ...)` | XML |
|  | `readHTMLTable("htmlfile", which= , ...)` |  |
| JSON | `fromJSON(   )`  | jsonlite |
| SPSS | `spss.get("SPSSfile.sav")` | Hmisc |
| Stata | `read.dta("Statafile.dta")` | foreign |
| SAS | `read.sas7bdat("SASfile.sas7bdat")` | sas7bdat |
| Database table(s) | `sqlFetch(CONNECTION, "TABLE NAME", ...)` | RODBC | 
|  | `sqlQuery(CONNECTION, "SQL QUERY", ...)` |  |
|  | `sqldf("SQL QUERY", dbname="database", ...)` | sqldf |


## Working with files

### Reading flat files from disk or the web

To import flat files, it's probably best to get in the habit of using `read.table` instead of `read.csv`. While comma-delimited files are indeed the most common, tab- and pipe-delimited can also be quite prevalent depending on the context. And, since `read.csv` is just a wrapper for `read.table`, you might save time in the long run by using read.table for any flat file, and changing the `sep=" "` value as needed, such as `sep=","` for comma, `sep="\t"` for tab, `sep="|"` for pipe, and so on. I also like to use the `stringsAsFactors=FALSE` option as well, as I often pull in data that has character fields, and it's easier to designate the factor variables later. Finally, `read.table` assumes that the data you're reading in doesn't have a header, so if there is one, you have to specify that with `header=TRUE`.

We saw reading in a semi-colon delimited flat file in the first chapter:

```
power = read.table("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
```

While reading in a flat file is pretty standard fare for any R user, it can sometimes require a little extra work to make sure it comes in correctly. One important option to remember is `na.strings`; while R will read missing data from blank cells as `NA` automatically, you find a wide variety of ways people code missing values, from `-99` to `?` and sometimes even '0'. Occasionally you find more than one `NA` designator in the same file, as you can see in the `power` dataset example, above.

Sometimes file encoding matters as well. This example downloads an online .csv file from the Canadian 2011 Public Service Employee Survey (PSES), using UTF-8 encoding to ensure that special characters are read in correctly:

```
# Read in a csv from the internet with non-ASCII characters
pses2011 = read.table("http://www.tbs-sct.gc.ca/pses-saff/2011/data/
  2011_results-resultats.csv", sep=",", encoding="UTF-8")
```

Note: this file has no header; in the next section, we'll download the Excel file with the documentation for this data and use that to create the header.


### Reading big files with data.table

The `data.table` package is extremely useful—and much, *much* faster than `read.table`—for larger files. The `data.table` function we used at the start of Chapter 1 on household power consumption took about 24 seconds on my laptop; the `fread` function below only took 2.3 seconds. 

```
require(data.table)
# If you want to read in a csv directly, use fread, e.g., if you
# had the raw pses2011 in a local csv, you'd read it in this way:
power = fread("Data/household_power_consumption.txt", sep=";", header=T, 
  na.strings=c("?",""), stringsAsFactors=FALSE)
```

If you *already* have a data frame and want to speed up reading and accessing it, use the `data.table` function:

````
power = data.table("power")
```


### Unzipping files within R

Larger files are often zipped up to save space and bandwidth. Using the `unz` function inside the usual `read.table` takes care of bringing a single file into R. We'll download the zip file for the [Bike Sharing dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) from the UCI Machine Learning Archive and unzip the daily use file from it.

```
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", "Bike-Data.zip")
bike_share_daily = read.table(unz("Bike-Data.zip", "day.csv"), header=T, quote="\"", sep=",")
```

If the zip file has more than one file in it, you can unzip them all from within R with `unzip`.

```
unzip("Bike-Data.zip")
```

Note: although there is an `untar` function, it is no longer necessary—gzipped files can be read directly into R with the standard `read.table` function.  

Once you have the data files (or other files) unzipped, you can bring them into R as data frames or other formats using the relevant "reading data" functions, e.g.: 

```
bike_share_daily = read.table("day.csv", header=T, sep=",")
bike_share_hourly = read.table("hour.csv", header=T, sep=",")
bike_share_readme = readLines("Readme.txt")
```


### Reading Excel files

To import Excel files, `readxl`, `gdata`, or `XLConnect` provide pretty much anything you'll need; `gdata` is faster and more powerful but requires Perl, so Windows users must ensure that's installed on their system before they can use it. `XLConnect` has a lot of options for those using Windows and/or are in Excel-heavy work environments, and while it can be slow with large files, it is based on Java so it could be easier to start with on Windows platforms (as long as the Java and R architectures match, i.e., if R is x64, Java should be as well). `readxl` is a new addition to the R ecosystem, and if all you need to do is pull in a worksheet as-is, it's by far the fastest and easiest method.

We'll use an Excel file that comes with the `readxl` package to demonstrate `readxl` and `XLConnect`.

```
# Load the package
require(readxl)

# Load the Excel workbook
datasets = system.file("extdata/datasets.xlsx", package = "readxl")

# Read in the first worksheet
iris_xl = read_excel(datasets, sheet = 1)
```

The `XLConnect` package makes it easy to read and write to and from Excel files. For sake of reproducibility and data permanency, you should always save data as plain text, e.g., as a csv, and not in proprietrary or non-text formats like Excel. So if you must write/save *into* Excel, `XLConnect` will do it... but you'll have to figure that out on your own. 

```
# Load the package
require(XLConnect)
 
# Load the Excel workbook
wb = loadWorkbook(system.file("extdata/datasets.xlsx", package = "readxl"))
```
 
If you want to read in a single tab/sheet:

```
# Read in the first tab
data_from_sheet = readWorksheet(wb, sheet = 1)
```

The `sheet=` option can use the sheet's name or its position, e.g., using `sheet="NAME OF SHEET"` in either `readxl` or `XLConnect` retrieves the same data.

If you want to read in *every* tab from a single Excel file:

```
# Get a list of worksheets in the workbook
sheets = getSheets(wb)

# Invisibly return each sheet as its own data frame
invisible(  
    lapply(sheets,function(sheet) 
        assign(sheet,readWorksheet (wb, sheet = sheet ), pos = 1))
    )
```

`XLConnect` doesn't support downloading an Excel file straight from the web. However, `gdata` can. You can use it to import the documentation file for the 2011 PSES dataset imported in the previous section, and since the sheet has non-data rows in it (at the bottom), we'll exclude those by using the `nrows` function. It also contains "multi-string" headers, so we'll use the `header`, `skip`, and `col.name` options to ignore their header and create our own:

```
# Load package
require(gdata)

# Download Excel file form web and read in the first sheet to a data frame
pses2011_header = read.xls("http://www.tbs-sct.gc.ca/pses-saff/2011/
  data/PSES2011_Documentation.xls", sheet="Layout-Format", nrows=22, 
  header=FALSE, skip=1, col.names=c("Variables", "Size", "Type", 
  "Description"), stringsAsFactors=FALSE)
```

A> Using the `colnames` function, the metadata we read in from this Excel file can be used to create the header names for the raw data we read in as a csv in the previous section:
A>
A> ~~~~~~~~
A> # Assign the Variable column names from the Excel file to the raw data file
A> colnames(pses2011) = pses2011_header$Variable
A> ~~~~~~~~


### Creating a dataframe from the clipboard or direct entry

A really useful short-cut to get spreadsheet or table data with a header row into R quickly is copying it into the clipboard from a spreadsheet and importing it via the `"clipboard"` option in the `read.table` function:

```
# Generic code for reading data from the clipboard
my_data_frame = read.table("clipboard", sep="\t", header=TRUE)
```

For a small amount of data, many R users would make vectors and create a data frame from them, e.g.,:

```
# Fake data in vectors
Survey_Response = c("Strongly Agree", "Agree", "Neutral", "Disagree", 
  "Strongly Disagree")
Send_Email_Ad = c("Yes", "Yes", "No", "No", "No")

# Combine vectors into a data frame
marketing = data.frame(Survey_Response, Send_Email_Ad)
```

Longtime SAS users might remember the datalines (or cards, if you're really an old-timer) method to create tables on the fly. While the clipboard method shown above is probably better in most cases, if you're used to using this old SAS method and you only need a quick, small, use-once table, the `textConnection` function could be handy to have in the toolbox:

```
# An R take on SAS's datalines
marketing = read.table(textConnection
("Survey_Response, Send_Email_Ad
Strongly Agree, Yes
Agree, Yes
Neutral, No
Disagree, No
Strongly Disagree, No"),
header=TRUE, sep=",")
```

Of course, whichever works best for you is the method to use.


### Reading XML files

We see a way to bring in specific tables from websites below, in the Web section, but this recipe provides an overview of bringing in a complete XML file. For a simple example, we'll use the 2010 Patient Satisfaction results from the Veterans Health Adminstration's [Hospital Report Card](http://www1.va.gov/health/HospitalReportCard.asp). 

```
require(XML)
# Read in the webpage as-is
vha_pt_sat_raw = readLines("http://www1.va.gov/VETDATA/docs/Datagov/Data_Gov_VHA_2010_Dataset11_Patient_Satisfaction.xml")

# Convert xml to a data frame
VHA_Patient_Satisfaction = xmlToDataFrame(vha_pt_sat_raw)
```

When there are more complex structures in the file, you may have to use another approach. Here we'll use the Federal Housing Finance Agency's [House Price Index (HPI)](http://www.fhfa.gov/DataTools/Downloads/Pages/House-Price-Index.aspx), which measures average price changes in repeat sales or refinancings on single-family homes. If you try the simple approach shown above, it will fail, with the warning `'data' must be of a vector type, was 'NULL'`. Going through a list first and then into a data frame can often make these types of xml files import successfully:

```
FHFA_HPI_raw = readLines("http://www.fhfa.gov/DataTools/Downloads/Documents/HPI/HPI_master.xml")
FHFA_HPI_list = xmlToList(xmlParse(FHFA_HPI_raw))
FHFA_HPI = data.frame(do.call(rbind, FHFA_HPI_list)) 
```


### Reading JSON files 

JSON data can be really useful for nested data, so it is becoming more popular for data visualizations (e.g., in d3-based web apps) and other applications where the usual row *x* column format would make for a large and unweidly data structure. Still, sometimes we need to work with these data structures in R, so reading them into a data frame is required. The `jsonlite` package is probably the best way to do this. 

```
# Load the jsonlite package
require(jsonlite)
# Get JSON data of UFO sightings
ufos = fromJSON("http://metricsgraphicsjs.org/data/ufo-sightings.json")
```

Usually, the `fromJSON` function will return a list if you're pulling in anything more complex than raw JSON, so you'll need to either call the data frame from within the list with a few `$`s, or convert that portion of the list into a data frame. We'll do the latter in this example:  

```
# Acquire the JSON data from Yahoo finance
currency_list = fromJSON("http://finance.yahoo.com/webservice/v1/symbols/allcurrencies/quote?format=json")

# This takes the piece of the list we need and brings it out as a data frame
currency_data = currency_list$list$resources$resource$fields

# Look at the data structure
str(currency_data)
```

![](images/str_json.png)

You'll notice that every variable is in character format, so you'll need to do a little cleaning to get this data into a format we can analyze. We'll see more on changing formats in the next chapter. 

Sometimes you encounter streaming JSON (aka "pseudo-JSON"), which contains JSON formatted-data but throws an error when you try to use the `fromJSON` function. Instead, use the `stream_in` function as follows:

```
# Download Enron emails to the working directory from jsonstudio
download.file("http://jsonstudio.com/wp-content/uploads/2014/02/enron.zip", "enron.zip")

# Unzip it
unzip("enron.zip")

# Bring in the streaming json as a data frame
enron = stream_in(file("enron.json"))
```

As above, each variable in the data frame is in `chr` format, so you'll have to convert to other data types where relevant (e.g., the `date` column). 


## Working with databases

### Connecting to a database

While those who work with server-based databases will typically manipulate the data in those environments before importing into R, there are times when having a direct link to the database can be useful—for ongoing and/or automated reporting, for one. This recipe covers the basics of `RODBC`, one of the packages that allows you to use ODBC to connect R to your database. I've found the `sqldf` package (see the next few recipes) easier to use for working with database tables once you've connected, but as usual there are a variety of options in R—choose the one that works best for your workflow.

As there are dozens of database platforms, all with their own system-specific drivers, we won't cover Data Source Name (DSN) setup in this book. Enterprise systems often have those already set up, but if not, your database admin and/or a search on stackoverflow can provide advice on setting up the proper drivers for your particular context for adding user or system DSN connections. As an example, this is how you might set up a user DSN in a Windows Server environment to access a SQL Server database:

1. Click the Start button. Type in data in the Search programs and files box and select Data Sources (ODBC).
2. Click Add.
3. Select SQL Server Native Client xx.x, or choose the appropriate driver if that's not the one you need. Click Finish.
4. Put the name you want to use to access the database in the Name box, e.g., `SQL3`. Put the actual name of the server in the Server box, e.g., perhaps this server is called `EDWSQLDB3`. Click Next.
5. Leave it as Integrated Windows Authentication unless for some reason you like doing things manually. Click Next.
6. Choose the database to match the specific one on the server you wish to access. If you have read/write privs to the data, leave that box as is. Otherwise, change `READWRITE` to `READONLY` in the dropdown list. Click Next.
7. Adjust settings as you prefer or just click Finish.
8. Click `Test Data Source`. Click OK. Click OK again to close the data source adminsitrator.

Once the appropriate DSNs are available for you, you can use the `RODBC` package to access your databases:

```
require(RODBC)
```

Once your system is configured for the ODBC connection, you need to define that connection within R, replacing the CAPS with your specific values:

```
# For example, if you'd followed the user DSN naming, above, 
# the "NAME OF DSN CONNECTION" would be "SQL3"
connection = odbcConnect("NAME OF DSN CONNECTION", uid="YOUR USER ID", 
    pwd="YOUR PASSWORD")
```

We can verify the connection is working:

```
odbcGetInfo(connection)
```

Once the connection is made, data can flow. If you don't already know the names of the tables or you want to verify their spelling, you can get a list by writing the list to an R object and then view that object:

```
# To see all tables:
sqlTables(connection)
# To see all tables in a given schema, e.g. dbo:
sqlTables(connection, schema="dbo")
```

If there are a lot of tables or you want to make a record of their names, create an object and then view it within R:

```
connection_tables = sqlTables(connection)
connection_tables
```

### Creating data frames from a database

Once you're connected to database, you can pull in an entire table using the `sqlFetch` function, but if you're using a database, you probably want to query it to get just what you need. To get a whole table, use `sqlFetch`:

```
# Fetch a table
whole_table = sqlFetch(connection, "table_name")
```

To make a SQL query and put the results into a dataframe, set up the query in an R object and then use the `sqlQuery` function. The following query is identical to above as it brings in the whole table:

```
# Within the double quotes you can put a valid SQL query,
# using single quotes to name the table
query_1 = "SELECT * FROM 'table_name'"
query_1_dataframe = sqlQuery(connection, query_1)

# While not as clear for large queries, you can combine them into one step:
query_1_dataframe = sqlQuery(connection, "SELECT * FROM 'table_name'")
```

Any typical SQL query will work; for example, this query will pull in all columns but only those records specified by the where clause, and will sort it by the order by variable(s):

```
query_2 = "SELECT * FROM 'table_name$' 
    WHERE variable_name = some_condition 
    ORDER BY ordering_variable_name"
query_2_dataframe = sqlQuery(connection, query_2)
```

### Disconnecting from a database

When you have the data you need, or at least by the end of the R session, close the database connection:

```
odbcCloseAll()
```

The `RODBC` package allows R to connect with any database that allows OBDC connections, and then interact with that database using SQL. Since database platforms, local systems, and server environments vary considerably, it can be difficult to specify how to set up the initial DSN connection for any given combination—you'll need to consult with a database administrator or search online for your particular combination of databases, drivers, and system connections. 
You can learn more by viewing: `vignette("RODBC", package="RODBC")`.


### Creating a SQLite database inside R

You don't even need to understand the intricacies of database planning and management to be able to create your own database on the fly from within R. While there's no need to do so if your data fits into R's memory, it can be really useful if you need to subset and merge several large files to obtain a working dataframe that can be comfortably analyzed within R's memory limits.

This recipe walks through creating a SQLite database and adding tables to it; the following recipe will walk through obtaining data from the database for analysis in R.

The `sqldf` package provides more options for database work without dealing with a formal database—everything can be done from within R.

```
require(sqldf)
```


Creating a new database with sqldf is easy:

```
sqldf("attach 'PSES_database.sqlite' as new")
```

It's just a shell at the moment; next create a connection to that database for use in subsequent table import and manipulation:

```
connect = dbConnect(SQLite(), dbname="PSES_database.sqlite")
```

And now you can read csv files directly into the database (i.e., so it is not throttled by passing through R first). We'll use the PSES 2011 file we downloaded in the flat files section earlier in this chapter.

```
read.csv.sql("pses2011.csv", sql = "CREATE TABLE pses2011 AS SELECT * FROM file",
    dbname = "PSES_database.sqlite")
```

Because an Excel file must be loaded into R first, and then passed to the database, you need to consider other options. If your Excel file has only a few sheets in it, saving them as csvs might be preferable. If you have many sheets to import, using `XLConnect` to import the sheets all at once could save a little time. They'll still need to be loaded into the database individually, however, and you should remove the dataframes when you've transferred them to the database.

```
# Download the Excel file
download.file("http://www.tbs-sct.gc.ca/pses-saff/2011/data/PSES2011_Documentation.xls", "PSES2011_Documentation.xls")

# Load the Excel file into R
pses2011_xls = loadWorkbook("PSES2011_Documentation.xls")
  
# Read each worksheet into separate dataframes within a list
pses2011_documentation = readWorksheet(pses2011_xls, 
    sheet=getSheets(pses2011_xls))
  
# The sheet names have spaces and hyphens, which will cause
# trouble for SQLite; run names(pses2011_documentation) to see
# So, this changes the dataframe names inside the list
names(pses2011_documentation) = c("Questions", "Agency", "LEVELID15", "Demcode",
    "PosNegSpecs", "LayoutFormat")
  
# Add a new row to account for 0 values for LEVEL1ID in
# main pses2011 file
pses2011_documentation$Agency = rbind(pses2011_documentation$Agency,
    c(0,NA,"Other",NA,"OTH"))
  
# Now each sheet can be loaded into the database as a separate table
with(pses2011_documentation, {
    dbWriteTable(conn=connect, name="Questions", value=Questions, 
      row.names=FALSE)
    dbWriteTable(conn=connect, name="Agency", value=Agency, 
      row.names=FALSE)
    dbWriteTable(conn=connect, name="LEVELID15", value=LEVELID15, 
      row.names=FALSE)
    dbWriteTable(conn=connect, name="Demcode", value=Demcode, 
      row.names=FALSE)
    dbWriteTable(conn=connect, name="PosNegSpecs", value=PosNegSpecs, 
      row.names=FALSE)
    dbWriteTable(conn=connect, name="LayoutFormat", value=LayoutFormat, 
      row.names=FALSE)   
} )
  
# Remove the Excel objects
rm(pses2011_xls, pses2011_documentation)
```

To see names of the tables in the database:

```
sqldf("SELECT * FROM sqlite_master", dbname = "PSES_database.sqlite")$tbl_name
```

To see table names and the SQL statements that generated them:

```
sqldf("SELECT * FROM sqlite_master", dbname = "PSES_database.sqlite")
```

To see names of columns in a particular table:

```
# Note: PRAMA and TABLE_INFO are SQLite-specific statements
sqldf("PRAGMA TABLE_INFO(Questions)", dbname = "PSES_database.sqlite")$name
```

A shortcut for seeing a list of tables and/or fields (which may not work depending on your system):    

```
dbListTables(connect)
dbListFields(connect, "Questions")  
```

The equivalent of `head` to look at the start of a table would be:

```
sqldf("SELECT * FROM Questions LIMIT 6", dbname = "PSES_database.sqlite")
```

Finally, whenever you're done interacting with the database, close the connection:

```
dbDisconnect(connect)
```

SQLite was built intentionally to a be "lightweight" database, and being able to set it up and access it from entirely within R makes it perfect for occasional to moderate desktop use. As is the case with the many different flavors of SQL, there are nuances in SQLite (as we saw with the PRAGMA statement, above), but most basic uses remain the same across those different types of SQL.

You can see a list of the SQL statements available with: 

```
.SQL92Keywords
```

More info on the way R interfaces with RDBMSs can be found on the homepage for the [DBI package](https://github.com/rstats-db/DBI), and details on SQLite can be found at its [homepage](http://www.sqlite.org/).  

Those with more sophisticated desktop database needs might consider [PostgreSQL](http://www.postgresql.org/), which also plays well with R via the `RPostgreSQL` package.


### Creating a dataframe from a SQLite database

Once you have a database connection, you're able to access exactly what you need and pull just that into R as a dataframe, conserving memory and saving time when working on projects that will need to access a variety of tables.

Load the `sqldf` package if it's not already loaded, and then reconnect to the database we created in the previous recipe:

```
require(sqldf)
connection = dbConnect(SQLite(), dbname="PSES_database.sqlite")
```

Selecting subsets from a database table using `sqldf` is based on (of course) SQL select statements. For example, this statement brings in three of the columns from the LEVEL1ID table:

```
pses_acr = sqldf("SELECT Level1ID, Acronym_PSES_Acronyme_SAFF AS Acronym, DeptNameE as Department FROM Agency", dbname="PSES_database.sqlite")
```

You can join tables within the database as well before bringing it into R. For example, this code merges the main data table (`pses2011`) with the agency name/abbreviation table to create a dataframe called `pses2011-agency`:

```
pses2011_agency = sqldf("SELECT
    maintable.*
    , agencytable.DeptNameE as Agency
    , agencytable.Acronym_PSES_Acronyme_SAFF as Abbr
    FROM pses2011 maintable
    LEFT JOIN Agency agencytable
        ON maintable.LEVEL1ID = agencytable.Level1ID",
    dbname="PSES_db.sqlite")
```

Don't forget to close the connection:  `dbDisconnect(connection)` or `close(connection)`.

Basically, `sqldf("SQL QUERY", dbname="NAME OF DATABASE")` is all you need to use SQLite within R, and it works on both databases and dataframes—if the latter, it's just `sqldf("SQL QUERY")`. It is important to note that unlike R, SQLite is not case sensitive, so be careful naming tables and columns—a and A are identical to SQLite. (SQL functions are in CAPS in the code examples in this book simply to distinguish them from table and column names, which are left in the same case that R sees them.)

You can do most basic types of SQL work in this fashion; for example, if you want to count the number of rows in your new dataframe where `ANSWER3` ("Neutral") is more than 50% of the total responses, have it grouped by `Question` and `Agency`, and order the result by highest to lowest counts, you can use:

```
agency_row_count = sqldf("SELECT
    Question
    , Agency
    , COUNT(ANSWER3) as Answer3_Count
    FROM pses2011_agency
    WHERE ANSWER3 > 50
    GROUP BY Question, Agency
    ORDER BY Answer3_Count desc")  
```

The possibilities are fairly limitless by containing large files inside a database and bringing in only what's needed for analysis using SQL, such as via `sqldf` or `RODBC`. A basic knowledge of SQL is required, of course, but the learning curve is very small for R users and well worth taking on given its centrality in data access and manipulation. It also serves as a great bridge for those used to SQL who want to use R more extensively but are not yet accomplished in using the R functions that have SQL equivalents (e.g., R's `merge` vs. SQL joins). For example, the standard SQL `CASE WHEN` statement could be used to generate new columns that sum the number of cases in which the majority either strongly agreed or strongly disagreed:

```
agency_row_count = sqldf("SELECT
    Question
    , SUM(CASE WHEN ANSWER1 > 51 THEN 1 ELSE 0 END) AS Strong_Agreement
    , SUM(CASE WHEN ANSWER5 > 51 THEN 1 ELSE 0 END) AS Strong_Disagreement
    FROM pses2011_agency
    GROUP BY Question, Agency
    ORDER BY Question desc")  
```

It's worth pointing out that if your data fits in memory, there usually isn't need for a database; merging and manipulation can occur inside R—recipes in the next chapter explore ways to do that.


## Getting data from the web

### Working through a proxy

If you're working behind a company firewall, you may have to use a proxy to pull data into R from the web. The `httr` package makes it simple.

```
require(httr)
```

Enter your proxy address, the port (usually 8080), and your user name/password in place of the CAPS code:

```
set_config(use_proxy(url="YOUR_PROXY_URL", port="YOUR_PORT", 
    username="YOUR_USER_NAME", password="YOUR_PASSWORD"))
```

There is a bewildering array of methods to access websites from within R, particularly while having to pass through a proxy, and most of them are obscure to even the most established quants. Thanks to `httr`, all of that complexity has been hidden behind a few simple functions—type `vignette("quickstart", package="httr")` if you want more information.

### Scraping data from a web table

Sometimes, webpages have a treasure trove of data in tables... but don't have an option to download it as a text or Excel file. And while data scraping addins are avilable for modern web browsers like Chrome (*Scraper*) or Firefox (*Table2Clipboard* and *TableTools2*) that make it as easy as point-and-click, if the tables aren't set up for easy download or a simple cut-and-paste, you can use the `XML` package to grab what you need.

While there are a few R packages that allow scraping, the `XML` package is the simplest to begin with, and may serve all your needs anyway.

```
require(XML)
```

As a simple example, we can explore the [2010 National Survey on Drug Use and Health](http://oas.samhsa.gov/NSDUH/2k10NSDUH/tabs/Sect1peTabs1to46.htm). Like many websites, it has pages that contain multiple tables. Say we just want Table 1.1A. First, read in the raw html:

```
drug_use_2010_table = readLines("http://oas.samhsa.gov/NSDUH/2k10NSDUH/tabs/Sect1peTabs1to46.htm")
```

Then read in the first table (`which=1`) part of the html:

```
drug_use_table1_1 = readHTMLTable(druguse, header=T, which=1, 
    stringsAsFactors=FALSE)
```

What if you want more than one table? Since we've read in the entire webpage, we can scrape it to extract whatever information we need. For example, let's say we want tables 1.17A and 1.17B. Using the webpage's [table of contents](http://oas.samhsa.gov/NSDUH/2k10NSDUH/tabs/TOC.htm#TopOfPage), we see that the tables we want are 31st and 32nd:

```
drug_use_table1_17a = readHTMLTable(druguse, header=T, which=31, 
    stringsAsFactors=FALSE)
drug_use_table1_17b = readHTMLTable(druguse, header=T, which=32, 
    stringsAsFactors=FALSE)
```

#### Scraping tables that cover multiple pages

What about a case where you need a table that goes over many pages? For example, ProPublica has a website that lists [deficiencies in nursing home care in the United States](http://projects.propublica.org/nursing-homes/); the subset that includes Texas includes more than 3,400 rows that cover 189 different (web)pages.  

Obviously, cutting and pasting that would seriously suck.

Here's how you can do it in R with a few lines of code. First, set up an R object that will iteratively list every one of the 189 web URLs:

```
allpages = paste("http://projects.propublica.org/nursing-homes/findings/search?page=", 1:189, "&search=&ss=ALL&state=TX", sep="")
```

then read in each page into an R list format:

```
tablelist = list()
for(i in seq_along(allpages)){
  page = allpages[i]
  page = readLines(page)
  homes = readHTMLTable(page, header=T, which=1, stringsAsFactors = FALSE)
  tablelist[[i]] = homes
  }
```

and finally turn the list into a data frame:

```
nursing_home_deficiencies = do.call(rbind, lapply(tablelist, data.frame, 
    stringsAsFactors=FALSE))
```

This result still needs some cleaning before we can move to analysis; Chapter 3 provides an example of cleaning this particular scrape in addition to the more general recipes. 


### Working with APIs

Many data-oriented sites have APIs that make it easy to pull in data to R. Some are wide open, but most true APIs  require keys. And of course, each one has its own terms of service and ways to implement. 

A good example of a typical API is via the US Census Bureau's American Community Survey (ACS). First, go to their [*Terms of Service* page](http://www.census.gov/data/developers/about/terms-of-service.html), and if you agree with those terms, click the **Request a KEY** button at the bottom of the left side menu bar. A pop-up will ask for your organization and email address (and agreement to the *Terms*, of course), and it will return a key to that email within a few minutes. 

The `acs` R package provides the link between the API and R, and also provides a function to permanently install the key so you don't have to enter it every time you hit the API:

```
require(acs)
api.key.install(key="y0uR K3y H3rE")
```

The ACS is *vast*. Since this is simply a recipe for example's sake, we'll assume you know the table and geographies you want data on... say, population by county in Texas. `acs` keeps the data frame part of the object in the `estimate` slot:

```
tx_pops_acs = acs.fetch(geography=geo.make(state="TX", county="Harris"), 
    table.number="B01003")
tx_county_pops = data.frame(tx_pops_acs@estimate)
```

We'll see more use of the `acs` package in the *Maps* section of Chapter 7. 


## Creating fake data to test code

Creating fake data is useful when you want to test how code will work before doing it "for real" on larger, more complicated datasets. We'll create a few fake dataframes in this recipe as both an example as how to do it, as well as for use in other recipes in this book.  

While creating and using fake data is useful for lots of cases, it is especially useful for merging/joining (which we'll explore in the next chapter). Sometimes joins may not behave like you expect that they will due to intricacies in your data, so testing it first on known (fake) data helps you determine whether any problems arose because of the code, or because of something in your data. If the fake data merge as expected but the real data don't, then you know that there's something in your data (and not the code!) that is messing with the join process. In large, enterprise-scale databases, it is typically the joins that can cause the most unexpected behavior—that is, can lead to the wrong results, impacting everything downstream of that join for the worse.

Whatever the use case, testing code on fake data can sometimes save considerable time that would have been lost debugging.

Using a variety of sources, we'll develop four dataframes that mimic a customer sales type of database (of course, these datasets are all small, so a database is unnecessary here).

```
# Table A – customer data
set.seed(1235813)
customer_id = seq(1:10000)
customer_gender = sample(c("M","F", "Unknown"), 10000, replace=TRUE,
  prob=c(.45, .45, .10))
customer_age = round(runif(10000, 18, 88), 0)
make_NAs = which(customer_age %in% sample(customer_age, 25))
customer_age[make_NAs] = NA
customer_purchases = round(rlnorm(10000)*100, 2)
  
# Table B – city/state/zip and business density lookup table
download.file("ftp://ftp.census.gov/econ2012/CBP_CSV/zbp12totals.zip", 
  "temp.zip", method="curl")
zipcodes = read.table(unz("temp.zip", "zbp12totals.txt"), header=T, quote="\"",
  sep=",", colClasses="character")
zipcodes = zipcodes[,c(1,11:12,10)]
# End of Table B
  
# Table A, continued
customer_zip = zipcodes[sample(1:nrow(zipcodes), 10000, replace=TRUE),]$zip
customers = data.frame(customer_id, customer_gender, customer_age,
  customer_purchases, customer_zip)
  
# TABLE C – results of a product interest survey
ones = seq(1, 1, length.out = 2000)
zeros = seq(0, 0, length.out = 2000)
strongly_agree = c(ones, zeros, zeros, zeros, zeros)
agree = c(zeros, ones, zeros, zeros, zeros)
neutral = c(zeros, zeros, ones, zeros, zeros)
disagree = c(zeros, zeros, zeros, ones, zeros)
strongly_disagree = c(zeros, zeros, zeros, zeros, ones)
survey = data.frame(customer_id, strongly_agree, agree, neutral, disagree,
  strongly_disagree)
  
# TABLE D – lookup table to match states to regions
state_regions = data.frame(datasets::state.abb, datasets::state.name, 
  datasets::state.region, datasets::state.division)
colnames(state_regions) = c("abb", "state", "region", "division")
```

In Table A, we've created a customer demographic and sales dataset. The `sample` function creates a random vector of male and female genders, as well as unknown, proportional to the weights in the `prob` option. The age and purchase values were created using the random number generators based on the uniform and log-normal distributions, respectively. The age value vector was subsequently replaced with 25% NAs. Each customer's zip code was generated from a random selection of the zip codes in Table B. All vectors are then brought together into the `customers` data frame. 

Table B, we've downloaded and unzipped some 2012 County Business Patterns data from the US Census to create a "locations" lookup table. (Metadata for this dataset is online [here](https://www.census.gov/econ/cbp/download/noise_layout/ZIP_Totals_Layout10.txt); in addition to the city/state/zip fields, we've also kept the `est` field, which is the number of business establishments in this zip code in the first quarter of 2012.)

Table C is just a systematic selection designating a survey response from each customer. This could also be done randomly, but is done this way here simply for illustraion. 

Finally, Table D takes advantage of R's built-in data from the `datasets` package to generate a lookup table of states and state regions.

I> `wakefield` is a new R package at the time of this writing that will generate random datasets for you. In addition to the random data frame generation, it also has a nice function to visualize the distribution of NAs in a data frame. Check out [its GitHub site](https://github.com/trinker/wakefield) for more info.


## Writing files to disk

There is usually no reason to save a dataframe for use outside R in any format other than a flat file: the only way to provide truly reproducible analytics is to ensure everything can be read by any program, language, or platform. 

`write.table` using `sep=","` will save your dataframe as a csv, and setting `row.names=FALSE` will save it without the row numbers that R provides by default.

```
write.table(pses2011, "pses2011.csv", sep=",", row.names=FALSE)
```

If you want to save data in JSON format, use `jsonlite` with the `pretty` option set to true to have it formatted cleanly. Then `sink` it into a file in the working directory.

```
iris_json = jsonlite::toJSON(iris, pretty=TRUE)
sink("iris_json.json")
iris_json
sink()
```

Note you can save memory by calling a function from a package using the `::` connector, which will run that function without having to load its home package. This can also be useful if you don't want to mask functions in already-loaded packages that have the same name. 
